{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate file with nearby cities\n",
    "Provide the min, max search range and the start city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  latitude  longitude  population  distance_km  \\\n",
      "3145             Schaerbeek  50.86935    4.37737      132761   301.113828   \n",
      "3241                  Namur  50.46690    4.86746      110939   327.521131   \n",
      "3255                   Mons  50.45413    3.95229       95299   356.029516   \n",
      "3258   Molenbeek-Saint-Jean  50.84990    4.31248       97037   305.224081   \n",
      "3315                 Leuven  50.87959    4.70093      101032   289.726190   \n",
      "...                     ...       ...        ...         ...          ...   \n",
      "40985               Legnica  51.21006   16.16190      106033   690.197837   \n",
      "41010              Koszalin  54.19438   16.17222      107450   640.929128   \n",
      "41071   Gorzów Wielkopolski  52.73679   15.22878      124430   582.067038   \n",
      "46184             Jönköping  57.78145   14.15618       93797   696.218783   \n",
      "46198           Helsingborg  56.04673   12.69437      104250   504.058693   \n",
      "\n",
      "      country code  \n",
      "3145            BE  \n",
      "3241            BE  \n",
      "3255            BE  \n",
      "3258            BE  \n",
      "3315            BE  \n",
      "...            ...  \n",
      "40985           PL  \n",
      "41010           PL  \n",
      "41071           PL  \n",
      "46184           SE  \n",
      "46198           SE  \n",
      "\n",
      "[141 rows x 6 columns]\n",
      "Nearby cities saved to nearby_cities.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import radians, cos, sin, sqrt, atan2\n",
    "\n",
    "# Parameters\n",
    "city = \"Groningen\"\n",
    "min_radius_km = 200\n",
    "max_radius_km = 700\n",
    "\n",
    "# Load the GeoNames data\n",
    "file_path = \"cities5000.txt\"  # Update with the actual file path\n",
    "columns = [\n",
    "    \"geonameid\", \"name\", \"asciiname\", \"alternatenames\", \"latitude\", \"longitude\", \n",
    "    \"feature class\", \"feature code\", \"country code\", \"cc2\", \"admin1 code\", \n",
    "    \"admin2 code\", \"admin3 code\", \"admin4 code\", \"population\", \"elevation\", \n",
    "    \"dem\", \"timezone\", \"modification date\"\n",
    "]\n",
    "\n",
    "# Load the file into a pandas DataFrame\n",
    "cities = pd.read_csv(file_path, sep='\\t', names=columns, low_memory=False)\n",
    "\n",
    "# Haversine function to calculate distance between two lat/lon pairs\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of Earth in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Function to find cities within a radius\n",
    "def find_cities_within_radius(lat, lon, min_radius_km, max_radius_km, min_population=0, max_population=10_000_000):\n",
    "    # Filter cities by population range\n",
    "    cities_in_population_range = cities[(cities['population'] >= min_population) & (cities['population'] <= max_population)]\n",
    "    cities['distance_km'] = cities_in_population_range.apply(\n",
    "        lambda row: haversine(lat, lon, row['latitude'], row['longitude']), axis=1\n",
    "    )\n",
    "    nearby_cities = cities[(cities['distance_km'] <= max_radius_km) & (cities['distance_km'] >= min_radius_km) & (cities['population'] >= min_population)]\n",
    "    return nearby_cities[['name', 'latitude', 'longitude', 'population', 'distance_km', 'country code']]\n",
    "\n",
    "def get_coordinates_by_city_name(city_name, country_code=None):\n",
    "    \"\"\"\n",
    "    Returns the latitude and longitude of a city given its name.\n",
    "    \n",
    "    Parameters:\n",
    "    - city_name (str): The name of the city to search for.\n",
    "    - country_code (str): Optional. ISO 3166-1 alpha-2 country code to narrow down the search.\n",
    "    \n",
    "    Returns:\n",
    "    - (float, float): A tuple containing (latitude, longitude) if found.\n",
    "    - None: If the city is not found.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame by city name (case insensitive)\n",
    "    filtered = cities[cities['name'].str.lower() == city_name.lower()]\n",
    "\n",
    "    # Optionally filter by country code\n",
    "    if country_code:\n",
    "        filtered = filtered[filtered['country code'].str.upper() == country_code.upper()]\n",
    "\n",
    "    if not filtered.empty:\n",
    "        # Get the first matching result\n",
    "        city_data = filtered.iloc[0]\n",
    "        return city_data['latitude'], city_data['longitude']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "latitude, longitude = get_coordinates_by_city_name(city)  # Example coordinates for Paris\n",
    "nearby_cities = find_cities_within_radius(latitude, longitude, min_radius_km, max_radius_km, 90_000, 150_000)\n",
    "\n",
    "# Display the result\n",
    "print(nearby_cities)\n",
    "# Save the nearby cities to a text file\n",
    "output_file_path = \"nearby_cities.csv\"\n",
    "nearby_cities.to_csv(output_file_path, sep='\\t', index=False)\n",
    "print(f\"Nearby cities saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filter the generated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name  latitude  longitude  population  distance_km country code\n",
      "9       Zwickau  50.72724   12.48839       98796   490.992852           DE\n",
      "10     Würzburg  49.79391    9.95121      133731   447.024137           DE\n",
      "11    Wolfsburg  52.42452   10.78150      123064   296.625588           DE\n",
      "12       Witten  51.44362    7.35258      101247   204.523657           DE\n",
      "13  Wilmersdorf  52.48333   13.31667      101877   460.399418           DE\n",
      "Size of filtered_df: (63, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the nearby_cities.csv file\n",
    "nearby_cities_df = pd.read_csv('nearby_cities.csv', sep='\\t')\n",
    "\n",
    "# Filter the dataframe to keep only entries where country code is in the list\n",
    "filtered_countries = ['FR', 'DE', 'DK', 'LU']\n",
    "filtered_df = nearby_cities_df[nearby_cities_df['country code'].isin(filtered_countries)]\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(filtered_df.head())\n",
    "print(f\"Size of filtered_df: {filtered_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "def fetch_hotel_prices_and_links(city, max_price=1000, group_adults=24, checkin=\"2025-03-07\", checkout=\"2025-03-09\", breakfast_included=False, save_file = False):\n",
    "    # or use your browser of choice\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    if breakfast_included:\n",
    "        driver.get(f\"https://www.booking.com/searchresults.en-gb.html?&ss={city}&ssne={city}&ssne_untouched={city}&lang=en-gb&sb=1&src_elem=sb&dest_type=city&checkin={checkin}&checkout={checkout}&group_adults={group_adults}&no_rooms=1&group_children=0&sr_view=list&order=price&nflt=mealplan%3D1\")\n",
    "    else:\n",
    "        driver.get(f\"https://www.booking.com/searchresults.en-gb.html?&ss={city}&ssne={city}&ssne_untouched={city}&lang=en-gb&sb=1&src_elem=sb&dest_type=city&checkin={checkin}&checkout={checkout}&group_adults={group_adults}&no_rooms=1&group_children=0&sr_view=list&order=price\")\n",
    "\n",
    "    prices = driver.find_elements(By.CSS_SELECTOR, 'span[data-testid=\"price-and-discounted-price\"]')\n",
    "    hotels = []\n",
    "    prices = driver.find_elements(By.CSS_SELECTOR, 'span[data-testid=\"price-and-discounted-price\"]')\n",
    "\n",
    "    if save_file:\n",
    "        with open(f\"./saved_pages/{city}.html\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(driver.page_source)\n",
    "\n",
    "    for i in range(len(prices)):\n",
    "        retry_count = 3  # Retry a few times in case of a stale element\n",
    "        while retry_count > 0:\n",
    "            try:\n",
    "                prices = driver.find_elements(By.CSS_SELECTOR, 'span[data-testid=\"price-and-discounted-price\"]')\n",
    "                price = prices[i]\n",
    "\n",
    "                price_text = price.text.replace(\"€\", \"\").replace(\",\", \"\").strip()\n",
    "                price_value = float(price_text)\n",
    "\n",
    "                if price_value < max_price:\n",
    "                    try:\n",
    "                        # 6 divs higher should always still be in the selected hotel's div\n",
    "                        parent_div = price.find_element(By.XPATH, './ancestor::div[6]')\n",
    "                        availability_button = parent_div.find_element(By.CSS_SELECTOR, 'a[data-testid=\"availability-cta-btn\"]')\n",
    "\n",
    "                        # If button is found, retrieve the link\n",
    "                        link = availability_button.get_attribute('href')\n",
    "                        hotels.append({\"price\": price_value, \"link\": link})\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        # Handle cases where neither attempt finds the button\n",
    "                        print(\"Availability button not found for this listing.\")\n",
    "                        hotels.append({\"price\": price_value, \"link\": \"No link available\"})\n",
    "\n",
    "                break  # Break out of the retry loop if successful\n",
    "            except StaleElementReferenceException:\n",
    "                retry_count -= 1\n",
    "                if retry_count == 0:\n",
    "                    print(f\"Failed to process price due to stale element: {i}\")\n",
    "            except TimeoutException:\n",
    "                retry_count -= 1\n",
    "                if retry_count == 0:\n",
    "                    print(\"Availability button did not appear in time.\")\n",
    "                    hotels.append({\"price\": price_value, \"link\": \"No link available\"})\n",
    "\n",
    "    driver.quit()\n",
    "    return hotels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multithreaded search\n",
    "Be careful when using this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel hotel data fetching...\n",
      "Fetching data for Zwickau...\n",
      "Fetching data for Würzburg...\n",
      "Fetching data for Wolfsburg...\n",
      "Fetching data for Witten...\n",
      "Fetching data for Wilmersdorf...\n",
      "Fetching data for Ulm...\n",
      "Fetching data for Trier...\n",
      "Fetching data for Siegen...\n",
      "Fetching data for Schwerin...\n",
      "Fetching data for Schöneberg...\n",
      "Fetching data for Salzgitter...\n",
      "Fetching data for Reutlingen...\n",
      "Fetching data for Remscheid...\n",
      "Fetching data for Regensburg...\n",
      "Fetching data for Ratingen...\n",
      "Fetching data for Rahlstedt...\n",
      "Fetching data for Prenzlauer Berg...\n",
      "Fetching data for Porz am Rhein...\n",
      "Fetching data for Pforzheim...\n",
      "Fetching data for Paderborn...\n",
      "Fetching data for Offenbach...\n",
      "Fetching data for Nippes...\n",
      "Fetching data for Marzahn...\n",
      "Fetching data for Koblenz...\n",
      "Fetching data for Kaiserslautern...\n",
      "Fetching data for Jena...\n",
      "Fetching data for Iserlohn...\n",
      "Fetching data for Ingolstadt...\n",
      "Fetching data for Hildesheim...\n",
      "Fetching data for Heilbronn...\n",
      "Fetching data for Heidelberg...\n",
      "Fetching data for Göttingen...\n",
      "Fetching data for Gesundbrunnen...\n",
      "Fetching data for Gera...\n",
      "Fetching data for Fürth...\n",
      "Fetching data for Friedrichshain...\n",
      "Fetching data for Esslingen...\n",
      "Fetching data for Erlangen...\n",
      "Fetching data for Düren...\n",
      "Fetching data for Darmstadt...\n",
      "Fetching data for Charlottenburg...\n",
      "Fetching data for Bergisch Gladbach...\n",
      "Fetching data for Mitte...\n",
      "Fetching data for Bergedorf...\n",
      "Fetching data for Rodenkirchen...\n",
      "Fetching data for Frederiksberg...\n",
      "Fetching data for Aalborg...\n",
      "Fetching data for Tourcoing...\n",
      "Fetching data for Saint-Denis...\n",
      "Fetching data for Roubaix...\n",
      "Fetching data for Orléans...\n",
      "Fetching data for Nancy...\n",
      "Fetching data for Mulhouse...\n",
      "Fetching data for Montreuil...\n",
      "Fetching data for Metz...\n",
      "Fetching data for Caen...\n",
      "Fetching data for Boulogne-Billancourt...\n",
      "Fetching data for Besançon...\n",
      "Fetching data for Argenteuil...\n",
      "Fetching data for Amiens...\n",
      "Fetching data for Saint-Quentin-en-Yvelines...\n",
      "Fetching data for Paris 11e Arrondissement...\n",
      "Fetching data for Paris 12e Arrondissement...\n",
      "Data fetching complete. Check 'hotels_breakfast.csv' for results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Parameters\n",
    "max_price = 1200\n",
    "breakfast_included = False\n",
    "checkin = \"2025-03-07\"   # Format: YYYY-MM-DD\n",
    "checkout = \"2025-03-09\"    # Format: YYYY-MM-DD\n",
    "people_count = 24\n",
    "\n",
    "# Wrapper function for fetching data\n",
    "def fetch_and_save(city_list, max_price=1000, group_adults=24, checkin=\"2025-03-07\", checkout=\"2025-03-09\", breakfast_included=False, save_file = False):\n",
    "    # Ensure the output file exists and has the correct headers\n",
    "    save_file_path = 'hotels_small.csv'\n",
    "    if breakfast_included:\n",
    "        save_file_path = 'hotels_breakfast_small.csv'\n",
    "    \n",
    "    if not os.path.exists(save_file_path):\n",
    "        with open(save_file_path, 'w') as file:\n",
    "            file.write(\"City\\tPrice\\tLink\\n\")\n",
    "\n",
    "    def worker(city):\n",
    "        # Fetch hotel prices and links for a single city\n",
    "        print(f\"Fetching data for {city}...\")\n",
    "        return {city: fetch_hotel_prices_and_links(city, max_price, group_adults, checkin, checkout, breakfast_included, save_file)}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit all city requests\n",
    "        future_to_city = {executor.submit(worker, city): city for city in city_list}\n",
    "\n",
    "        # Process results as they complete\n",
    "        for future in as_completed(future_to_city):\n",
    "            city = future_to_city[future]\n",
    "            try:\n",
    "                data = future.result()  # Get the result\n",
    "                hotels = data[city]\n",
    "\n",
    "                # Append results to the file\n",
    "                with open(save_file_path, 'a', encoding='utf-8') as file:\n",
    "                    for hotel in hotels:\n",
    "                        file.write(f\"{city}\\t{hotel['price']}\\t{hotel['link']}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {city}: {e}\")\n",
    "\n",
    "city_list = filtered_df['name']\n",
    "\n",
    "print(\"Starting parallel hotel data fetching...\")\n",
    "fetch_and_save(city_list, max_price=max_price, group_adults=people_count, checkin=checkin, checkout=checkout, breakfast_included=breakfast_included)\n",
    "\n",
    "save_file_path = 'hotels.csv'\n",
    "if breakfast_included:\n",
    "    save_file_path = 'hotels_breakfast.csv'\n",
    "    \n",
    "print(f\"Data fetching complete. Check '{save_file_path}' for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Single thread search\n",
    "Is slow, but will go undetected for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Parameters\n",
    "max_price = 1000\n",
    "breakfast_included = False\n",
    "checkin = \"2025-03-07\"   # Format: YYYY-MM-DD\n",
    "checkout = \"2025-03-09\"    # Format: YYYY-MM-DD\n",
    "people_count = 24\n",
    "\n",
    "\n",
    "if not os.path.exists('hotels.csv'):\n",
    "    with open('hotels.csv', 'w') as file:\n",
    "        file.write(\"City\\tPrice\\tLink\\n\")\n",
    "        \n",
    "for city in filtered_df['name']:\n",
    "    print(f\"Fetching prices for {city}\")\n",
    "    hotels = fetch_hotel_prices_and_links(city, max_price=max_price, group_adults=people_count, checkin=checkin, checkout=checkout, breakfast_included=breakfast_included)\n",
    "    with open('hotels.csv', 'a', encoding='utf-8') as file:\n",
    "        for hotel in hotels:\n",
    "            file.write(f\"{city}\\t{hotel['price']}\\t{hotel['link']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create list of unique cities\n",
    "This can be loaded in [Google Maps](https://www.google.com/maps/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('hotels_info.txt'):\n",
    "    hotels_df = pd.read_csv('hotels_info.txt', sep=',')\n",
    "    print(hotels_df.head())\n",
    "    unique_cities = hotels_df['City'].unique()\n",
    "    print(unique_cities)\n",
    "    unique_cities_df = pd.DataFrame(unique_cities, columns=['City'])\n",
    "    unique_cities_df.to_csv('unique_cities.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
